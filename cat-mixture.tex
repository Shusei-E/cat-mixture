\documentclass[11pt]{article}
\usepackage{sk_memo}

% Parmaters --
\title{ \Large\textbf{Clustering Voter Types with Multinomial Outcomes}}
\author{\normalsize  Shiro Kuriwaki\thanks{Thanks to Shusei Eshima, Max Goplerud, Sooahn Shin, and Soichiro Yamauchi for their generous time and help.} }

\date{\normalsize October 2019}


\begin{document}
\maketitle

\onehalfspacing


\section{Data Generating Process}


\paragraph{Setup}

Index individuals by \(i \in \{1, ..., N\}\) and the universe of races
excluding the top of the ticket as \(j \in \{1, ..., D\}\). The data we
observe is a length-\(D\) vector of votes \(\bY_i\). \(Y_{ij}\) is a
discrete response value, \(Y_{ij} \in \{0, ..., L\}\). 

In the simplest case \(L = 1\), we code each vote \(y_{ij}\) an indicator for splitting their ticket or not. \(Y_{ij} = 1\) would mean voter \(i\) splitting their ticket in some office \(j\), with reference to a top of the ticket office like the President or Governor. When \(L = 2\), we can consider three outcomes: abstention \(Y_{ij} = 0\), split \(Y_{ij} = 0\), or straight \(Y_{ij} = 2\).


\paragraph{Parameters}


There are two sets of parameters: \(\bmu\), the propensity for a given outcome for a given type of voter in a given office; and \(\bth\), the mixing proportions of each type.  Individuals are endowed with a cluster (or type) \(k \in \{1, ..., K\}\), which is drawn from a distribution governed by length-\(K\) simplex \(\bth\) (the mixing proportion).
\begin{align*}
Z_i \sim  \text{Cat}(\bth),
\end{align*}

Set \(\mu_{k, j} \in [0, 1]\) be the parameter that governs the outcome of each type. Therefore \(\bmu\) is a \(\{K \times D \times (L + 1)\}\) array, where 
 \[\pr{Y_{ij} = \ell \mid Z_i = k} = \mu_{kj\ell}.\]

In other words, for each individual (who is  type \(k\)), their observed vector \(\bY_{i}\) is governed by a length-\(D\) parameter \(\bmu_{k}\). Therefore, we can express the joint density as follows. 
\begin{align}
\pr{\bY_{i} \mid Z_{i} = k, \bth} = \prod^{D}_{j = 1} \text{Cat}(Y_{ij} | \bmu_k) = \prod^{D}_{j = 1}\prod^{L}_{\ell=0} \mu_{jk\ell}^{\ind{Y_{ij} = \ell}}
\end{align}

The loop over \(\ell\) simply represents the categorical distribution. In the binary case, the Categorical reduces to a Bernoulli:

\begin{align}
\pr{\bY_{i} \mid Z_{i} = k, \bth} = \prod^{D}_{j=1}\mu_{jk}^{Y_{ij}}(1 - \mu_{jk})^{1- Y_{ij}}\nonumber
\end{align}




The benefit of this modeling exercise over that from a naive sample of
\(N \times D\) Bernoullis is that we have captured the correlations
between variables.\footnote{That dependency can be expressed as \(\mathds{E}(\by) = \sum_{k = 1}^{K} \theta_{k} \bmu_{k}\) and 
\(\cov({\by}) = \sum_{k} \theta_{k} (\mathbf{\Sigma}_k \bmu_k\bmu_{k}^{\top}) - \mathds{E}(\by)\mathds{E}(\by)^\top\), where \(\Sigma_k = \text{diag}(\mu_{jk}(1 - \mu_{jk}))\).}


\section{Clustering as an Unobserved Variable Problem: EM}

This mixture model lends itself to clustering analysis such as \(K\)-means. Although we can estimate this model in a Bayesian fashion by setting a prior for \(\bth\) and \(\bmu\), the \textsf{Stan} program cannot reliably estimate clustering models like this one by MCMC because of label-switching and multimodality. 

Because the model is simple enough, we can derive an algorithm to obtain the global solution for the parameters.\footnote{Thanks to Soichiro Yamauchi for deriving this algorithm in the original iteration.} An EM implementation makes it possible to handle extensions, such as systematic missing data, multinomial outcomes, and covariates. 

\paragraph{Complete Likelihood} If we knew the cluster assignment, we would be able to write the complete log-likelihood (\(\mathcal{L}_{\text{comp}}\)). First start with the joint probability of the outcome data and the cluster assignment:
\begin{align}
\Pr(\bY, \bZ \mid \bmu, \bth) &= \Pr(\bY \mid \bZ, \bmu, \bth)\Pr(\bZ \mid \bth)\nonumber\\
&= \prod^{N}_{i=1}\prod^{D}_{j=1}\Pr(Y_{ij} \mid \bZ, \bmu) \prod^N_{i=1}\Pr(Z_i \mid \bth)\nonumber\\
&= \prod^{N}_{i=1}\prod^{D}_{j=1}\prod^{K}_{k=1}\left\{\prod^{L}_{\ell=0}\pr{Y_{ij} = \ell | Z_i = k}\right\}^{\ind{Z_i=k}} \prod^N_{i=1}\prod^K_{k=1}\pr{Z_i = {k} \mid \bth}^{\ind{Z_i = k}}\nonumber \\
&= \prod^{N}_{i=1}\prod^{D}_{j=1}\prod^{K}_{k=1}\prod^{L}_{\ell=0}\pr{Y_{ij} = \ell \mid Z_{i} = k, \bmu}^{\ind{Y_{ij} = \ell, Z_i = k}} \prod^N_{i=1}\prod^K_{k=1}\pr{Z_i = {k} \mid \bth}^{\ind{Z_i = k}}\nonumber
\end{align}

Therefore, the complete log-likelihood is computed by taking the log of this:
\begin{align}
\mathcal{L}_{\text{comp}}(\bmu, \bth|\bY, \bZ)
& = \sum^{N}_{i=1}\sum^{D}_{j=1}\sum^{K}_{k=1}\sum^{L}_{\ell = 0}
\bm{1}\{Y_{ij} = \ell, Z_{i} = k\}\Big\{
\log \Pr(Y_{ij} = \ell | Z_{i} = k, \bm{\mu})
\Big\} \nonumber\\
& \qquad +
\sum^N_{i=1}\sum^{K}_{k=1}\bm{1}\{Z_{i} = k\}\log \Pr(Z_{i} = k | \bm{\theta})
\end{align}

We first take expectations over the latent variable $Z_{i}$,
\begin{align}
\E{\mathcal{L}_{\text{comp}}}
& = \sum^{N}_{i=1}\sum^{D}_{j=1}\sum^{K}_{k=1}\sum^{L}_{\ell = 0}
\bm{1}\{Y_{ij} = \ell\} \E{\bm{1}\{Z_{i} = k\}}\Big\{
\underbrace{\log \Pr(Y_{ij} = \ell | Z_{i} = k, \bm{\mu})}_{\equiv \log\bm{\mu}_{kj\ell}}
\Big\} \nonumber\\
& \qquad +
\sum^{N}_{i=1}\sum^{K}_{k=1}\E{\bm{1}\{Z_{i} = k\}}\underbrace{\log \Pr(Z_{i} = k | \bm{\theta})}_{= \log \bm{\theta}_{k}}
\end{align}

 Let's define this unknown quantity as 
\[\zeta_{ik} \equiv \E{\ind{Z_i = k}}.\]

Then the E-step can be the normalized version of the posterior probability marginalized by the mixing proportion,

\begin{align}
\widehat{\zeta}_{ij} \propto \bth_k\prod^D_{j = 1}\underbrace{\prod^{L}_{\ell = 0}\left(\bmu_{kj,\ell}\right)^{\ind{Y_{ij} = \ell}}}_{\bmu_{kj, Y_{ij}}}
\end{align}

The M-step is derived by taking the derivatives of \(\E{\mathcal{L}_{\text{comp}}}\) with respect to the model parameters \(\bmu\) and \(\bth\). This leads to a MLE-like M-step

\begin{align}
\widehat{\theta}_{k} &= \frac{1}{N}\sum^N_{i = 1}\zeta_{ik}\\
\widehat{\mu}_{jk\ell} &= \frac{\sum^N_{i=1}\ind{Y_{ij} = \ell}\zeta_{ik}}{\sum^{N}_{i=1}\zeta_{ik}}
\end{align}


\paragraph{EM Implementation} We first need to set initial values for \(\bmu\) and \(\bth\). I do this by letting \(\bth^{(0)} = \left(\frac{1}{K},...\frac{1}{K}\right)\), randomly assigning an initial cluster assignment \(Z_i^\prime \sim \text{Cat}(\bth^{(0)})\), and setting the initial \(\bmu\) by the sample means of the data within those initial assignments, \(\mu_{jk}^{(0)} = \frac{\sum_{i=1}^{N}\ind{Y_{ij} = 1}\ind{Z_{i}^{\prime} = k}}{\sum_{i=1}^{N}\ind{Z_{i}^{\prime} = k}}.\) 

Then we iterate as follows. For each voter \(i\), compute the probability that they belong in cluster \(k\) (E-step):

\begin{align}
\zeta_{ik} \leftarrow \frac{\bm{\theta}_{k}\prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}}}
{\sum^{K}_{k^\prime=1}\bm{\theta}_{k^\prime}\prod^{D}_{j=1}\bm{\mu}_{k^\prime j,Y_{ij}}}
\end{align}

Then given those type probabilities, update the parameters with the MLE (M-step)

\begin{align}
\text{for each \(k\), update: }~~~  \widehat{\theta}_{k} &\leftarrow \frac{1}{N}\sum^N_{i = 1}\widehat{\zeta}_{ik} \\
\text{for each \(k, j, \ell\), update: }~~~ \widehat{\mu}_{jk\ell} &\leftarrow\frac{\sum^N_{i=1}\ind{Y_{ij} = \ell}\widehat{\zeta}_{ik}}{\sum^{N}_{i=1}\widehat{\zeta}_{ik}}
\end{align}

We repeat this until convergence.

\paragraph{Evaluating Convergence} We evaluate convergence by the observed log likelihood, 

\begin{align*}
\mathbf{L}_{\text{obs}} = \prod^N_{i=1}\sum^{K}_{k=1}\theta_k \prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}}
\end{align*}

So the log-likelihood is
\begin{align}
\mathcal{L}_{\text{obs}} = \sum^{N}_{i=1}\log \left\{\sum^{K}_{k=1}\theta_k \prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}}\right\} \label{eq:obsloglik}
\end{align}

Calculating eq. (\ref{eq:obsloglik}) is computationally intensive, so a quick way to check convergence is to track the maximum of the change in parameters which are all on the probability scale, i.e. \(\max\left\{|\widehat\theta^{(t + 1)}_{1} - \widehat\theta^{(t)}_{1}|, ..., |\widehat\mu^{(t + 1)}_{K,D} - \widehat\mu^{(t)}_{K,D}|\right\}\)

\paragraph{Speed-Ups} Because this EM algorithm deals with discrete data, the algorithm needs only sufficient statistics. In our setting the unique number of voting profiles is much smaller than the number of observations, because vote vectors follow a systematic pattern and most votes are straight-ticket votes. Therefore, we can re-format the dataset so that each row is a unique combination.

Let \(u \in \{1, ..., U\}\) index the unique voting profiles, and \(n_{u}\) be the number of such profiles in the data.  We re-cycle the objects \(\bY\) and \(\bm\zeta\) so that each row indexes profiles rather than voters.

We repeat the EM algorithm described earlier. For each profile \(u\), compute the probability that it belong in type \(k\):

\begin{align}
\text{for each \(u, k\), update: }~~~   \widehat\zeta_{uk} \leftarrow \frac{\bm{\theta}_{k}\prod^{D}_{j=1}\bm{\mu}_{kj,Y_{uj}}}
{\sum^{K}_{k^\prime=1}\bm{\theta}_{k^\prime}\prod^{D}_{j=1}\bm{\mu}_{k^\prime j,Y_{uj}}}
\end{align}

Then given those type probabilities, update with

\begin{align}
\text{for each \(k\), update: }~~~   \widehat{\theta}_{k} &\leftarrow \frac{1}{N}\sum^U_{u = 1}n_{u}\widehat{\zeta}_{uk}\\
\text{for each \(k, j, \ell\), update: }~~~   \widehat{\mu}_{jk\ell} &\leftarrow\frac{\sum^U_{u=1}n_{u}\ind{Y_{uj} = \ell}\widehat{\zeta}_{uk}}{\sum^{U}_{u=1}n_{u}\widehat{\zeta}_{uk}}
\end{align}

And the log-likelihood will also only require looping through the profiles:

So the log-likelihood is
\begin{align}
\mathcal{L}_{\text{obs}} &= \sum^{U}_{u=1}\log n_u + \sum^{U}_{u=1}\log\left\{\sum^{K}_{k=1}\theta_k \prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}}\right\}
\end{align}


\section{Modeling Uncontested Races} 

A majority of elections for state and local offices are uncontested, which means that a voter technically votes in a choice but does not have the option to run for one of the candidates. These choices are qualitatively different from contested races in their data generating process. 

Consider the trichotomous outcome \(L = 2\) where \(Y_{ij} = 0\) indicates abstention, \(Y_{ij} = 1\) indicates ticket splitting (appearing like they are voting for their less preferred party as expressed in the top of the ticket) and \(Y_{ij} = 2\) indicates co-partisan voting.  

Voters for a given office now fall into one of these three categories, denoted by \(M_{ij} \in \{1, 2, 3\}\). Let \(M_{ij} = 3\) denote the contested case, so the voter has three options. Let \(M_{ij} = 2\) denote the case when only the preferred party candidate is in the race, so the voter has options \(Y_{ij} \in \{0, 2\}\). Finally let \(M_{ij} = 1\) denote the case when only the opposed party candidate is in the race, so the voter only has the option to abstain or ``reluctantly''  (as it might seem) vote for the less favored option by splitting: \(Y_{i} \in \{0, 1\}\).

To express the choice probability for option \(\ell\) for office \(j\) among voters of type \(k\), let us introduce another parameter \(\bps\) which represents the intensity of preference for option \(\ell \in \{1, 2\}\) relative to \(\ell = 0\) (abstention). We set the baseline for abstention to be 0, i.e. \(\psi_{jk,(\ell=0)} = 0 ~\forall~ j, k\).  Then the log-likelihood can be expressed by 

\begin{align}
\mu_{jk\ell} &= \ind{M_{ij}= 1}\frac{\exp{\psi_{kj\ell}}}{\sum_{\ell^\prime \in \{0, 1\}}\exp{\psi_{kj\ell^\prime}}}\nonumber \\
&\qquad +\ind{M_{ij}= 2}\frac{\exp{\psi_{kj\ell}}}{\sum_{\ell^\prime \in \{0, 2\}}\exp{\psi_{kj\ell^\prime}}}\nonumber \\
&\qquad\qquad +\ind{M_{ij}= 3}\frac{\exp{\psi_{kj\ell}}}{\sum_{\ell^\prime \in \{0, 1, 2\}}\exp{\psi_{kj\ell^\prime}}}\label{eq:missing}
\end{align}


\textbf{Remark}: Because \(\exp(\psi_{jk\ell}) = 1\) for \(\ell = 0\), which exists in all three components, each component is analogous to a simple multinomial logit. In the first two cases, since we consider only two possibilities, it reduces to a simple intercept-only logit. Also notice that we use the same set of parameters \(\bps_{jk}\) regardless of \(M_{ij}\). This represents the well-known independence of irrelevant alternatives (IIA) assumption in multinomial logit. The choice probabilities when one option is not on the ``menu'' is assumed to follow the same type of decision rule as the ratio between the existing options.  We can therefore think of equation \ref{eq:missing} as representing that the relevant parameter \(\mu\) for each individual takes one of the types depending on the (known) class of menu options, without introducing more parameters into the model.


\end{document}

