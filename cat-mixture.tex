\documentclass[11pt]{article}
\usepackage{sk_memo}

% Parmaters --
\title{ \Large\textbf{Clustering Voter Types with Multinomial Outcomes}}
\author{\normalsize  Shiro Kuriwaki\thanks{Thanks to Shusei Eshima, Max Goplerud, Sooahn Shin, and Soichiro Yamauchi for their generous time and help.} }

\date{\normalsize October 2019}


\begin{document}
\maketitle

\onehalfspacing


\section{Data Generating Process}


\paragraph{Setup}

Index individuals by \(i \in \{1, ..., N\}\) and the universe of races
excluding the top of the ticket as \(j \in \{1, ..., D\}\). The data we
observe is a length-\(D\) vector of votes \(\bY_i\). \(Y_{ij}\) is a
discrete response value, \(Y_{ij} \in \{0, ..., L\}\). 

In the simplest case \(L = 1\), we code each vote \(Y_{ij}\) an indicator for splitting their ticket or not. \(Y_{ij} = 1\) would mean voter \(i\) splitting their ticket in some office \(j\), with reference to a top of the ticket office like the President or Governor. In the multinomial. case of \(L = 2\), which will be our default setting, we can consider three outcomes: \(Y_{ij} = 0\) indicates \emph{abstention}, \(Y_{ij} = 1\) indicates ticket \emph{splitting} and \(Y_{ij} = 2\) indicates \emph{straight} (co-party) voting.  


\paragraph{Parameters}


There are two sets of parameters: \(\bmu\), the propensity for a given outcome for a given type of voter in a given office; and \(\bth\), the mixing proportions of each type.  Individuals are endowed with a cluster (or type) \(k \in \{1, ..., K\}\), which is drawn from a distribution governed by length-\(K\) simplex \(\bth\) (the mixing proportion).
\begin{align*}
Z_i \sim  \text{Cat}(\bth),
\end{align*}

Set \(\mu_{kj\ell} \in [0, 1]\) be the probability parameter that governs the probability of a given outcome for a given office, by a given type of voter. That is, \(\bmu\) is a \(\{K \times D \times (L + 1)\}\) array, where 
 \[\pr{Y_{ij} = \ell \mid Z_i = k} = \mu_{kj\ell}.\]

In other words, for each individual (who is  type \(k\)), their observed vector \(\bY_{i}\) is governed by a length-\(D\) parameter \(\bmu_{k}\). Therefore, we can express the joint density as follows. 
\begin{align}
\pr{\bY_{i} \mid Z_{i} = k, \bth} = \prod^{D}_{j = 1} \text{Cat}(Y_{ij} | \bmu_k) = \prod^{D}_{j = 1}\prod^{L}_{\ell=0} \mu_{kj\ell}^{\ind{Y_{ij} = \ell}}
\end{align}
The loop over \(\ell\) simply represents the categorical distribution. In the binary case of \(L = 1\), the Categorical reduces to a Bernoulli:
\begin{align}
\pr{\bY_{i} \mid Z_{i} = k, \bth} = \prod^{D}_{j=1}\mu_{kj}^{Y_{ij}}(1 - \mu_{kj})^{1- Y_{ij}}\nonumber
\end{align}

The benefit of this modeling exercise over that from a naive sample of
\(N \times D\) Bernoullis is that we have captured the correlations
between variables.


\section{Clustering as an Unobserved Variable Problem: EM}

This mixture model lends itself to clustering analysis such as \(K\)-means clustering. Although we can estimate this model in a Bayesian fashion by setting a prior for \(\bth\) and \(\bmu\), the MCMC sampler Stan cannot reliably estimate clustering models like this one because of label-switching and multimodality. 

Instead, we can derive the EM algorithm that is guarateed to recover the (local) maximum likelihood estimates for the target parameters. Unlike off-the-shelf algorithms like \(K\)-means, an EM approach can handle extensions such as discrete and unordered multinomial outcomes, systematic missing data, and covariates. The rest of this paper outlines the EM algorithm.\footnote{Thanks to Soichiro Yamauchi for deriving the original iteration of this algorithm for me.}

\paragraph{Complete Likelihood} If we knew the cluster assignment, we would be able to write the complete log-likelihood (\(\mathcal{L}_{\text{comp}}\)). First start with the joint probability of the outcome data and the cluster assignment:
\begin{align}
\Pr(\bY, \bZ \mid \bmu, \bth) &= \Pr(\bY \mid \bZ, \bmu, \bth)\Pr(\bZ \mid \bth)\nonumber\\
&= \prod^{N}_{i=1}\prod^{D}_{j=1}\Pr(Y_{ij} \mid \bZ, \bmu) \prod^N_{i=1}\Pr(Z_i \mid \bth)\nonumber\\
&= \prod^{N}_{i=1}\prod^{D}_{j=1}\prod^{K}_{k=1}\left\{\prod^{L}_{\ell=0}\pr{Y_{ij} = \ell | Z_i = k}^{\ind{Y_{ij} = \ell}}\right\}^{\ind{Z_i=k}} \prod^N_{i=1}\prod^K_{k=1}\pr{Z_i = {k} \mid \bth}^{\ind{Z_i = k}}\nonumber
\end{align}

Therefore, the complete log-likelihood is computed by taking the log of this:
\begin{align}
\mathcal{L}_{\text{comp}}(\bmu, \bth|\bY, \bZ)
& = \sum^{N}_{i=1}\sum^{D}_{j=1}\sum^{K}_{k=1}\sum^{L}_{\ell = 0}
\bm{1}\{Y_{ij} = \ell, Z_{i} = k\}
\log \Pr(Y_{ij} = \ell | Z_{i} = k, \bm{\mu})\nonumber\\
& \qquad +
\sum^N_{i=1}\sum^{K}_{k=1}\bm{1}\{Z_{i} = k\}\log \Pr(Z_{i} = k | \bm{\theta})
\end{align}

We first take expectations over the latent variable $Z_{i}$,
\begin{align}
\E{\mathcal{L}_{\text{comp}}}
& = \sum^{N}_{i=1}\sum^{D}_{j=1}\sum^{K}_{k=1}\sum^{L}_{\ell = 0}
\ind{Y_{ij} = \ell} \E{\ind{Z_{i} = k}}
\underbrace{\log \Pr(Y_{ij} = \ell | Z_{i} = k, \bm{\mu})}_{\equiv \log\bm{\mu}_{kj\ell}}
\nonumber\\
& \qquad +
\sum^{N}_{i=1}\sum^{K}_{k=1}\E{\ind{Z_{i} = k}}\underbrace{\log \Pr(Z_{i} = k | \bm{\theta})}_{= \log \bm{\theta}_{k}} \label{eq:Elik}
\end{align}

 Let's define this unknown quantity as 
\[\zeta_{ik} \equiv \E{\ind{Z_i = k}}.\]

Then the E-step can be the normalized version of the posterior probability marginalized by the mixing proportion,
\begin{align}
\widehat{\zeta}_{ik} \propto \theta_k\prod^D_{j = 1}\underbrace{\prod^{L}_{\ell = 0}\left(\mu_{kj\ell}\right)^{\ind{Y_{ij} = \ell}}}_{\equiv \bmu_{kj, Y_{ij}}}
\end{align}

The M-step is derived by taking the derivatives of \(\E{\mathcal{L}_{\text{comp}}}\) with respect to the model parameters \(\bmu\) and \(\bth\). This leads to a MLE-like M-step, which is shown in the next section (derivation in appendix).


\paragraph{EM Implementation} We first need to set initial values for \(\bmu\) and \(\bth\). I do this by letting \(\bth^{(0)} = \left(\frac{1}{K},...\frac{1}{K}\right)\), randomly assigning an initial cluster assignment \(Z_i^\prime \sim \text{Cat}(\bth^{(0)})\), and setting the initial \(\bmu\) by the sample means of the data within those initial assignments, \(\mu_{kj}^{(0)} = \frac{\sum_{i=1}^{N}\ind{Y_{ij} = 1}\ind{Z_{i}^{\prime} = k}}{\sum_{i=1}^{N}\ind{Z_{i}^{\prime} = k}}.\) 

Then we iterate as follows. For each voter \(i\), compute the probability that they belong in cluster \(k\) (E-step):
\begin{align}
\zeta_{ik} \leftarrow \frac{\bm{\theta}_{k}\prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}}}
{\sum^{K}_{k^\prime=1}\bm{\theta}_{k^\prime}\prod^{D}_{j=1}\bm{\mu}_{k^\prime j,Y_{ij}}}
\end{align}

Given those type probabilities, we update the parameters in the M-step. That will show that for updating \(\theta_k\), we should take the simple average of \(\widehat\zeta_{ik}\) across all \(i\). For updating \(\widehat\mu_{kj\ell}\), we should take for each \(k\) and \(\ell\) the sample proportion of the occurrence of \(Y_{ij} = \ell\), but weighted by \(\widehat\zeta_{ik}\):
\begin{align}
\text{for each \(k\), update: }~~~  \widehat{\theta}_{k} &\leftarrow \frac{1}{N}\sum^N_{i = 1}\widehat{\zeta}_{ik} \\
\text{for each \(k, j, \ell\), update: }~~~ \widehat{\mu}_{kj\ell} &\leftarrow\frac{\sum^N_{i=1}\ind{Y_{ij} = \ell}\widehat{\zeta}_{ik}}{\sum^{N}_{i=1}\widehat{\zeta}_{ik}},
\end{align}
repeated  until convergence.

\paragraph{Evaluating Convergence} We evaluate convergence by the observed log likelihood, 

\begin{align*}
\mathbf{L}_{\text{obs}} = \prod^N_{i=1}\sum^{K}_{k=1}\theta_k \prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}}
\end{align*}
So the observed log-likelihood is
\begin{align}
\mathcal{L}_{\text{obs}} = \sum^{N}_{i=1}\log \left\{\sum^{K}_{k=1}\theta_k \prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}}\right\} = \sum^{N}_{i=1}\log \left\{\sum^{K}_{k=1}\theta_k \prod^{D}_{j=1}\prod^{L}_{\ell = 0}\left(\mu_{kj\ell}\right)^{\ind{Y_{ij} = \ell}}\right\} \label{eq:obsloglik}
\end{align}

Calculating eq. \ref{eq:obsloglik} is computationally intensive, so a quick way to check convergence is to track the maximum of the change in parameters which are all on the probability scale, i.e. \(\max\left\{|\widehat\theta^{(t + 1)}_{1} - \widehat\theta^{(t)}_{1}|, ..., |\widehat\mu^{(t + 1)}_{K,D} - \widehat\mu^{(t)}_{K,D}|\right\}\)

\paragraph{Speed-Ups} Because this EM algorithm deals with discrete data, the algorithm needs only sufficient statistics. In our setting the unique number of voting profiles is much smaller than the number of observations, because vote vectors follow a systematic pattern and most votes are straight-ticket votes. Therefore, we can re-format the dataset so that each row is a unique combination.

Let \(u \in \{1, ..., U\}\) index the unique voting profiles, and \(n_{u}\) be the number of such profiles in the data.  We re-cycle the objects \(\bY\) and \(\bm\zeta\) so that each row indexes profiles rather than voters.

We repeat the EM algorithm described earlier. For each profile \(u\), compute the probability that it belong in type \(k\):

\begin{align}
\text{for each \(u, k\), update: }~~~   \widehat\zeta_{uk} \leftarrow \frac{\bm{\theta}_{k}\prod^{D}_{j=1}\bm{\mu}_{kj,Y_{uj}}}
{\sum^{K}_{k^\prime=1}\bm{\theta}_{k^\prime}\prod^{D}_{j=1}\bm{\mu}_{k^\prime j,Y_{uj}}}
\end{align}

Then given those type probabilities, update with

\begin{align}
\text{for each \(k\), update: }~~~   \widehat{\theta}_{k} &\leftarrow \frac{1}{N}\sum^U_{u = 1}n_{u}\widehat{\zeta}_{uk}\\
\text{for each \(k, j, \ell\), update: }~~~   \widehat{\mu}_{kj\ell} &\leftarrow\frac{\sum^U_{u=1}n_{u}\ind{Y_{uj} = \ell}\widehat{\zeta}_{uk}}{\sum^{U}_{u=1}n_{u}\widehat{\zeta}_{uk}}
\end{align}

And the observed log-likelihood will also only require looping through the profiles:
\begin{align}
\mathcal{L}_{\text{obs}} &= \sum^{U}_{u=1}\log n_u + \sum^{U}_{u=1}\log\left\{\sum^{K}_{k=1}\theta_k \prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}}\right\}
\end{align}


\section{Modeling Uncontested Races} 

A majority of elections for state and local offices are uncontested, which means that a voter technically votes in a choice but does not have the option to vote for one of the candidates. These qualitatively different settings require us to model \emph{varying choice sets}. 

\paragraph{Categories of uncontestedness} In uncontested races, some options are not available to choose from. To show this, we introduce a new layer: voter \(i\) for a given office \(j\) is in one of three settings, denoted by \(M_{ij} \in \{1, 2, 3\}\). Unlike the cluster \(Z_i\), that status is exactly observed in the data. 

Denote \(M_{ij} = 3\) to mean vote \(j\) for voter \(i\) falls in the \emph{contested} case, so the voter has all three options on the ``menu''. Denote \(M_{ij} = 2\) as the case when only the \emph{preferred party's} candidate is in the contest, so the voter only has options \(Y_{ij} \in \{0, 2\}\). Finally denote \(M_{ij} = 1\) as the case when only the \emph{opposed party's} candidate is in the contest, so the voter only has the option to abstain or reluctantly (perhaps) vote for the less favored option by splitting: \(Y_{i} \in \{0, 1\}\). For shorthand, I use the notation \(\bm{S}_{m}\) for the set of possible of values of \(Y_{ij}\) allowed for a given category of contestedness:
\begin{align*}
S_{m} = \begin{cases}
\{0, 1\} & \text{~if~} m = 1\\
\{0, 2\}  & \text{~if~} m = 2\\
\{0, 1, 2\} & \text{~if~} m = 3\\
\end{cases}
\end{align*}

Therefore the complete likelihood is modified by replacing the loop \(\ell = \{0, ..., L\}\) to \(\ell in S_m\).


\paragraph{Moving from a Multinomial model to a logit model} To express the choice probability for option \(\ell\) for office \(j\) among voters of type \(k\), let us introduce another parameter \(\bps\) which represents the intensity of preference for option \(\ell \in \{1, 2\}\) relative to \(\ell = 0\) (abstention). We set the baseline for abstention to be 0, i.e. \(\psi_{kj,(\ell=0)} = 0 ~\forall~ k, j\).  


\paragraph{Parameterization} To show the essence of this modeling choice, the below shows the case when there are no clusters \(K = 1\) and no latent heterogeneity across individuals for simplicity. Then the indices \(i, k\) drop and we get

\begin{align}
\mu_{j\ell} &= \ind{M_{j}= 1}\frac{\exp{(\psi_{j\ell}})}{\sum_{\ell^\prime \in \{0, 1\}}\exp{(\psi_{j\ell^\prime})}} +\ind{M_{j}= 2}\frac{\exp{(\psi_{j\ell})}}{\sum_{\ell^\prime \in \{0, 2\}}\exp{(\psi_{j\ell^\prime}})}\nonumber +\ind{M_{j}= 3}\frac{\exp{(\psi_{j\ell}})}{\sum_{\ell^\prime \in \{0, 1, 2\}}\exp{(\psi_{j\ell^\prime}})}\nonumber\\
&= \sum_{m = 1}^{3}\ind{M_{j}= m}\frac{\exp{(\psi_{j\ell})}}{\sum_{\ell^\prime \in S_m}\exp(\psi_{kj\ell^\prime})}\nonumber
\end{align}

Now we add the clustering and individual indices back in for the real data, effectively another layer to account for the fact that individuals are both of a type (\(Z_i\)) and each separate office is also of a missingness type \(M_{ij}\). 
\begin{align}
\mu_{kj\ell} &= \frac{1}{N\theta_k}\sum^N_{i = 1}\ind{Z_i = k, M_{ij}= m}\frac{\exp{(\psi_{kj\ell})}}{\sum_{\ell^\prime \in \bm{S}_{m}}\exp{(\psi_{kj\ell^\prime})}}\label{eq:missing}
\end{align}





% \begin{align}
% \mu_{kj\ell} &= \sum^K_{k=1}\ind{Z_i = k, M_{ij}= 1}\frac{\mu_{kj\ell}}{\sum_{\ell^\prime \in \{0, 1\}}\mu_{kj\ell^\prime}}\nonumber \\
% &\qquad +\sum^K_{k=1}\ind{Z_i = k, M_{ij}= 2}\frac{\mu_{kj\ell}}{\sum_{\ell^\prime \in \{0, 2\}}\mu_{kj\ell^\prime}}\nonumber \\
% &\qquad\qquad +\sum^K_{k=1}\ind{Z_i = k, M_{ij}= 3}\frac{\mu_{kj\ell}}{\sum_{\ell^\prime \in \{0, 1, 2\}} \mu_{kj\ell^\prime} }\label{eq:missing}
% \end{align}


\paragraph{Analog to Multinomial Logit} Because \(\exp(\psi_{kj\ell}) = 1\) for \(\ell = 0\), which exists in all three components, each component is analogous to a simple multinomial logit. In the first two cases, since we consider only two possibilities, it reduces to a simple intercept-only logit. Also notice that we use the same set of parameters \(\bps_{kj}\) regardless of \(M_{ij}\). This represents the well-known independence of irrelevant alternatives (IIA) assumption in multinomial logit. The choice probabilities when one option is not on the ``menu'' is assumed to follow the same type of decision rule as the ratio between the existing options.  

\paragraph{EM Estimation} We can use this new representation of the parameter \(\mu\) in the EM algorithm. The three components in equation \ref{eq:missing} can be cast as a multinomial logit. R packages of multinomial logit typically presume IIA if an outcome value is missing and implicitly do the kind of three-way subsetting as in equation \ref{eq:missing}. The required data would be of the ``long'' form shown in Table \ref{tab:choiceset}.

\begin{table}[!h]
\caption{How uncontested races affect choice sets \label{tab:choiceset}}
\centering
\singlespacing
\small
\begin{tabular}{cccccccc}
\toprule
\multicolumn{4}{c}{Voter level} & \multicolumn{2}{c}{Voter-office level} & \multicolumn{2}{c}{Choice}\\
\cmidrule(lr){1-4} \cmidrule(lr){5-6}\cmidrule(lr){7-8}
Voter     & \(\zeta_{i1}\) & \(\cdots\) & \(\zeta_{iK}\) &   Office & \(M_{ij}\) status & Option $\ell$ &  \(Y_{ij} = \ell\)\\\midrule
\(i = 1\) & 0.12 & \(\cdots\) & 0.05 &  \(j = 1\) & 3 (contested) & 0 & \texttt{FALSE}\\
\(i = 1\) & 0.12 & \(\cdots\) & 0.05 &  \(j = 1\) & 3 (contested) & 1 & \texttt{FALSE}\\
\(i = 1\) & 0.12 & \(\cdots\) & 0.05 &  \(j = 1\) & 3 (contested) & 2 & \texttt{TRUE}\\\addlinespace
\(i = 1\) & 0.12 & \(\cdots\) & 0.05 &  \(j = 2\) & 2 (cannot split) & 0 & \texttt{FALSE}\\
\(i = 1\) & 0.12 & \(\cdots\) & 0.05 &  \(j = 2\) & 2 (cannot split) & 1 & \textcolor{crimson}{\texttt{NA}}\\
\(i = 1\) & 0.12 & \(\cdots\) & 0.05 &  \(j = 2\) & 2 (cannot split) & 2 & \texttt{TRUE}\\\addlinespace
\(i = 1\) & 0.12 & \(\cdots\) & 0.05 &  \(j = 3\) & 1 (cannot straight) & 0 & \texttt{FALSE}\\
\(i = 1\) & 0.12 & \(\cdots\) & 0.05 &  \(j = 3\) & 1 (cannot straight) & 1 & \texttt{TRUE}\\
\(i = 1\) & 0.12 & \(\cdots\) & 0.05 &  \(j = 3\) & 1 (cannot straight) & 2 & \textcolor{crimson}{\texttt{NA}}\\\addlinespace
\(i = 2\) & 0.01 & \(\cdots\) & 0.80 &  \(j = 1\) & 1 (cannot straight) & 0 & \texttt{FALSE}\\
\(i = 2\) & 0.01 & \(\cdots\) & 0.80 &  \(j = 1\) & 1 (cannot straight) & 1 & \texttt{FALSE}\\
\(i = 2\) & 0.01 & \(\cdots\) & 0.80 &  \(j = 1\) & 1 (cannot straight) & 2 & \textcolor{crimson}{\texttt{NA}}\\
\bottomrule
\end{tabular}
\end{table}

We will estimate this regression separately for each \(k \in \{1, ..., K\}\), using the estimates of \(\bm{\zeta}_{k}\), which represents the posterior probability of voter \(i \in \{1, ..., K\}\) being in cluster \(k\), as the weights of the logit:
\begin{align}
\text{for each \(k\), update: }~~~  \widehat{\theta}_{k} &\leftarrow \frac{1}{N}\sum^N_{i = 1}\widehat{\zeta}_{ik} \\
\text{for each \(k, j, \ell\), update: }~~~ \widehat{\mu}_{kj\ell} &\leftarrow  \frac{\exp\left(\widehat\psi_{kj\ell}\right)}{1 + \exp\left(\widehat\psi_{kj1}\right) + \exp\left(\widehat\psi_{kj2}\right)}
\end{align}

Where the \(\bm\psi_{kj}\) vector is estimated from the coefficients of a multinomial logit, of the form 
\begin{align*}
\texttt{mlogit(Y[[j]] \(\sim\) 1, data, weights = zeta\_k)}.
\end{align*}

\paragraph{Evaluating Convergence} When following the EM algorithm on this data affected by uncontested choices, the observed log likelihood changes. Recall that in the no-missing case,  we have equation \ref{eq:obsloglik}. However, in cases of missingness, the contribution of a data point also depends on the contestedness class.

\begin{align*}
{\mathcal{L}}^{\star}_{\text{obs}} &= \sum^{N}_{i=1}\log \left[\sum^{K}_{k=1}\theta_k \prod^{D}_{j=1}\prod_{\ell \in S_{M_{ij}}}\left\{\left(\frac{\mu_{kj\ell}}{\sum_{\ell^\prime \in S_{M_{ij}}} \mu_{kj\ell^\prime}}\right)^{\ind{Y_{ij} = \ell}}\right\}\right]
\end{align*}


\newpage
\appendix

\begin{centering}
\textbf{\large{Appendix}}
\end{centering}

\section{Deriving EM with complete data}

Recall that the expectation of the likelihood from equation \ref{eq:Elik} is

\begin{align*}
\E{\mathcal{L}_{\text{comp}}} = \sum^{N}_{i=1}\sum^{D}_{j=1}\sum^{K}_{k=1}\sum^{L}_{\ell = 0}\ind{Y_{ij} = \ell} \zeta_{ik}\log\mu_{kj\ell} +  
\sum^{N}_{i=1}\sum^{K}_{k=1}\zeta_{ik}\log \theta_{k}
\end{align*}
so to optimize we introduce Langrange multipliers \(\lambda\) and \(\bm{\eta}\) for the constraints on \(\bth\) and \(\bmu_{kj}\), respectively:
\begin{align}
\widetilde{\mathcal{L}} = \E{\mathcal{L}_\text{comp}} - \lambda\left(\sum^{K}_{k=1}\bth_k - 1\right) - \sum^{K}_{k=1}\sum^{D}_{j=1}\eta_{kj}\left(\sum^{L}_{\ell=0}\mu_{kj\ell} - 1\right)\label{eq:lagrange_a}
\end{align}
Then, for \(\bth\) we have that
\begin{align*}
\frac{\partial}{\partial {\theta}_{k}}\widetilde{\mathcal{L}}
&= \frac{\sum^{N}_{i=1}\zeta_{ik}}{{\theta}_{k}} - \lambda  = 0
\end{align*}
along with the constraint \(\sum^K_{k=1}\theta_k = 1\). Notice that when we sum the FOC for \(\bth\) across \(k\), the first condition becomes \(\sum^K_{k=1}\theta_k = \frac{1}{\lambda}\sum^K_{k = 1}\sum^N_{i=i}\zeta_{ik}\), and because the LHS sums to 1 due to the constraint and in the RHS  \(\sum^{N}_{i=1}\sum^{K}_{k^\prime =1}\zeta_{ik^\prime}\) sums to \(N\), we have \(\lambda = N\). 

\medskip

Separately, for \(\bmu_{kj}\) we have that
\begin{align*}
\frac{\partial}{\partial {\mu}_{kj\ell}}\widetilde{\mathcal{L}}
&= \frac{\sum^{N}_{i=1}\ind{Y_{ij} = \ell}\zeta_{ik}}{\bm{\mu}_{kj\ell}} - \eta_{kj}  = 0,
\end{align*}
along with constraint \(\sum^L_{\ell = 0} \mu_{kj\ell} = 1\). Once we sum the FOC for \(\bmu\) across \(\ell\) the first condition becomes  \(\sum^L_{\ell=0} \mu_{kj\ell} = \frac{1}{\eta_{kj}}\sum_{i=1}\sum_{\ell=0}\ind{Y_{ij} = \ell}\zeta_{ik}\), and because the LHS again sums to 1 and in the RHS   \(\sum^{N}_{i=1}\sum^{L}_{\ell = 0}\ind{Y_{ij} = \ell}\zeta_{ik}\) sums to the prevalence of the weights \(\sum^{N}_{i=1}\zeta_{ik}\), we get \(\eta_{kj}= \sum^{N}_{i=1}\zeta_{ik}\).
\medskip

Together, the above imply that
\begin{equation}
\theta_{k} = \frac{1}{N}\sum^{N}_{i=1}\zeta_{ik}\quad
\text{and}\quad
\mu_{kj\ell} =
\frac{\sum^{N}_{i=1}\bm{1}\{Y_{ij} = \ell\}\zeta_{ik}}{\sum^{N}_{i=1}\zeta_{ik}}
\end{equation}


\section{Deriving EM with censored data}

The modified log likelihood is
% \begin{align}
% \Pr(\bY, \bZ \mid \bmu, \bth)\nonumber &= \prod^{N}_{i=1}\prod^{D}_{j=1}\prod^{K}_{k=1}\left\{\prod^{}_{\ell\in S_{M_{ij}}}\pr{Y_{ij} = \ell | Z_i = k}^{\ind{Y_{ij} = \ell}}\right\}^{\ind{Z_i=k}} \prod^N_{i=1}\prod^K_{k=1}\pr{Z_i = {k} \mid \bth}^{\ind{Z_i = k}}\nonumber
% \end{align}
\begin{align}
\mathcal{L}_{\text{comp}}(\bmu, \bth|\bY, \bZ) & = \sum^{N}_{i=1}\sum^{D}_{j=1}\sum^{K}_{k=1}\sum^{}_{\ell \in S_{M_{ij}}}
\ind{Y_{ij} = \ell, Z_{i} = k}
\log \Pr(Y_{ij} = \ell | Z_{i} = k,  M_{ij} = m, \bm{\mu})\nonumber\\
& \qquad +
\sum^N_{i=1}\sum^{K}_{k=1}\ind{Z_{i} = k}\log \Pr(Z_{i} = k | \bm{\theta})\nonumber
\end{align}
And the expected log likelihood, taking expectations over \(Z_i\) is 
\begin{align}
\E{\mathcal{L}_{\text{comp}}} &= \sum^{N}_{i=1}\sum^{D}_{j=1}\sum^{K}_{k=1}\sum^{L}_{\ell \in M_{ij}}\ind{Y_{ij} = \ell}\zeta_{ik}\underbrace{\log \Pr(Y_{ij} = \ell | Z_{i} = k, M_{ij} = m, \bm{\mu})}_{=\log \left(\frac{\mu_{kj\ell}}{\sum_{\ell^\prime \in S_{m}} \mu_{kj\ell}}\right)}\nonumber\\
& + \sum^{N}_{i=1}\sum^{K}_{k=1}\zeta_{ik}\log \bm{\theta}_{k}
\end{align}

\paragraph{E-step} Then the E-step can be the normalized version of the posterior probability marginalized by the mixing proportion,
\begin{align}
\widehat{\zeta}_{ik} \propto \theta_k\prod^{D}_{j = 1}\underbrace{\prod^{L}_{\ell \in S_m}\left(\frac{\mu_{kj\ell}}{\sum_{\ell^\prime \in S_{m}} \mu_{kj\ell^\prime}}\right)^{\ind{Y_{ij} = \ell}}}_{\equiv \bmu_{kj, Y_{ij}, M_{ij}}}
\end{align}
So in the E-step, we would be updating by this probability:
\begin{align}
\zeta_{ik} \leftarrow \frac{\bm{\theta}_{k}\prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}, M_{ij}}}
{\sum^{K}_{k^\prime=1}\bm{\theta}_{k^\prime}\prod^{D}_{j=1}\bm{\mu}_{k^\prime j,Y_{ij}, M_{ij}}}
\end{align}

\paragraph{M-step}  The M-step involves taking the derivative of one more layer of complication. Re-using notation we introduce Langrange multipliers \(\lambda\) and \(\bm{\eta}\) for the constraints on \(\bth\) and \(\bmu_{kj}\), respectively and modify eq. \ref{eq:lagrange_a} as:

\begin{align}
\widetilde{\mathcal{L}} = \E{\mathcal{L}_\text{comp}} - \lambda\left(\sum^{K}_{k=1}\bth_k - 1\right) - \sum^{K}_{k=1}\sum^{D}_{j=1}\eta_{kj}\left(\sum_{\ell \in S_{m}}\left(\frac{\mu_{kj\ell}}{\sum_{\ell^\prime \in S_{m}} \mu_{kj\ell^\prime}}\right) - 1\right) \nonumber
\end{align}

We can deduce from the structure of \(\bmu\) array, the equivalent thing to the E-step in the complete data case is to run a standard multinomial logit with a IIA assumption (also called conditional logit).



% \left(\frac{\exp(\psi_{kj\ell})}{\sum_{\ell \in S_m} \exp(\psi_{kj\ell^\prime})}\right)



% Suppose \(M_{ij} = m\). We decompose the likelihood in the \(L = 2\) case as 
% \begin{align*}
% \mathcal{L}_{\text{comp}} = \sum_{i=1}\sum_{j=1}\sum_{k=1}\Bigg\{&\ind{Y_{ij} = 0}\zeta_{ik}\log\frac{1}{\sum_{\ell^\prime \in S_{m}} \exp(\psi_{kj\ell^\prime})} +\\
% & \ind{Y_{ij} = 1}\zeta_{ik}\log\frac{\exp(\psi_{kj1})}{\sum_{\ell^\prime \in S_{m}} \exp(\psi_{kj\ell^\prime})} + \\
% &\qquad \ind{Y_{ij} = 2}\zeta_{ik}\log\frac{\exp(\psi_{kj2})}{\sum_{\ell^\prime \in S_{m}} \exp(\psi_{kj\ell^\prime})}\Bigg\}
% \end{align*}

% Taking the derivative of this with respect to \(\psi_{kj1}\), for example, will involve a cross-product. In the \(M_{ij} = m = 3\) case, we have


% \begin{align*}
% \frac{\partial \mathcal{L}_\text{comp}}{\partial \psi_{kj1}}  =& \sum_{i=1}\ind{Y_{ij} = 1}\zeta_{ik}\\
% &- \sum_{i=1}\ind{Y_{ij} = 1}\zeta_{ik}\frac{\exp(\psi_{kj1})}{\sum_{\ell^\prime \in S_{m}} \exp(\psi_{kj\ell^\prime})}\\
% &- \sum_{i=1}\ind{Y_{ij} = 0}\zeta_{ik}\frac{\exp(\psi_{kj1})}{\sum_{\ell^\prime \in S_{m}} \exp(\psi_{kj\ell^\prime})}\\
% &- \sum_{i=1}\ind{Y_{ij} = 2}\zeta_{ik}\frac{\exp(\psi_{kj1})}{\sum_{\ell^\prime \in S_{m}} \exp(\psi_{kj\ell^\prime})}\\
% =& \sum_{i=1}\ind{Y_{ij} = 1}\zeta_{ik} - \sum_{i}\zeta_{ik}\frac{\exp(\psi_{kj1})}{1 + \sum_{\ell^\prime \in S_m} \exp(\psi_{kj\ell^\prime})}
% \end{align*}
% Because \(\frac{d}{d\psi_{kj1}} \log(1 + \exp(\psi_{kj1}) + \exp(\psi_{kj2})) = \frac{\exp(\psi_{kj1})}{\sum_{\ell^\prime \in S_{m}} \exp(\psi_{kj\ell^\prime})}.\)

% When \(m = 1\), \(\ell = 2\) is not available so we need to drop that.

% 2019-10-16 this version takes 32 minutes for three EM models, two using IIA. Probably each IIA model takes about 15 minutes. Estimation log likelihood also took about 14 miniutes.

\end{document}


