\documentclass[11pt]{article}
\usepackage{sk_memo}

% Parmaters --
\title{ \Large\textbf{Clustering Voter Types with Multinomial Outcomes}}
\author{\normalsize  Shiro Kuriwaki\thanks{Thanks to Shusei Eshima, Max Goplerud, Sooahn Shin, and Soichiro Yamauchi for their generous time and help.} }

\date{\normalsize October 2019}


\begin{document}
\maketitle

\onehalfspacing


\section{Data Generating Process}


\paragraph{Setup}

Index individuals by \(i \in \{1, ..., N\}\) and the universe of races
excluding the top of the ticket as \(j \in \{1, ..., D\}\). The data we
observe is a length-\(D\) vector of votes \(\bY_i\). \(Y_{ij}\) is a
discrete response value, \(Y_{ij} \in \{0, ..., L\}\). 

In the simplest case \(L = 1\), we code each vote \(y_{ij}\) an indicator for splitting their ticket or not. \(Y_{ij} = 1\) would mean voter \(i\) splitting their ticket in some office \(j\), with reference to a top of the ticket office like the President or Governor. When \(L = 2\), we can consider three outcomes: abstention \(Y_{ij} = 0\), split \(Y_{ij} = 0\), or straight \(Y_{ij} = 2\).


\paragraph{Parameters}


There are two sets of parameters: \(\bmu\), the propensity for a given outcome for a given type of voter in a given office; and \(\bth\), the mixing proportions of each type.  Individuals are endowed with a cluster (or type) \(k \in \{1, ..., K\}\), which is drawn from a distribution governed by length-\(K\) simplex \(\bth\) (the mixing proportion).
\begin{align*}
Z_i \sim  \text{Cat}(\bth),
\end{align*}

Set \(\mu_{k, j} \in [0, 1]\) be the parameter that governs the outcome of each type. Therefore \(\bmu\) is a \(\{K \times D \times (L + 1)\}\) array, where 
 \[\pr{Y_{ij} = \ell \mid Z_i = k} = \mu_{kj\ell}.\]

In other words, for each individual (who is  type \(k\)), their observed vector \(\bY_{i}\) is governed by a length-\(D\) parameter \(\bmu_{k}\). Therefore, we can express the joint density as follows. 
\begin{align}
\pr{\bY_{i} \mid Z_{i} = k, \bth} = \prod^{D}_{j = 1} \text{Cat}(Y_{ij} | \bmu_k) = \prod^{D}_{j = 1}\prod^{L}_{\ell=0} \mu_{jk\ell}^{\ind{Y_{ij} = \ell}}
\end{align}

The loop over \(\ell\) simply represents the categorical distribution. In the binary case, the Categorical reduces to a Bernoulli:

\begin{align}
\pr{\bY_{i} \mid Z_{i} = k, \bth} = \prod^{D}_{j=1}\mu_{jk}^{Y_{ij}}(1 - \mu_{jk})^{1- Y_{ij}}\nonumber
\end{align}




The benefit of this modeling exercise over that from a naive sample of
\(N \times D\) Bernoullis is that we have captured the correlations
between variables.\footnote{That dependency can be expressed as \(\mathds{E}(\by) = \sum_{k = 1}^{K} \theta_{k} \bmu_{k}\) and 
\(\cov({\by}) = \sum_{k} \theta_{k} (\mathbf{\Sigma}_k \bmu_k\bmu_{k}^{\top}) - \mathds{E}(\by)\mathds{E}(\by)^\top\), where \(\Sigma_k = \text{diag}(\mu_{jk}(1 - \mu_{jk}))\).}


\section{Clustering as an Unobserved Variable Problem: EM}

This mixture model lends itself to clustering analysis such as \(K\)-means. Although we can estimate this model in a Bayesian fashion by setting a prior for \(\bth\) and \(\bmu\), the \textsf{Stan} program cannot reliably estimate clustering models like this one by MCMC because of label-switching and multimodality. 

Because the model is simple enough, we can derive an algorithm to obtain the global solution for the parameters.\footnote{Thanks to Soichiro Yamauchi for deriving this algorithm in the original iteration.} An EM implementation makes it possible to handle extensions, such as systematic missing data, multinomial outcomes, and covariates. 

\paragraph{Complete Likelihood} If we knew the cluster assignment, we would be able to write the complete log-likelihood (\(\mathcal{L}_{\text{comp}}\)). First start with the joint probability of the outcome data and the cluster assignment:
\begin{align}
\Pr(\bY, \bZ \mid \bmu, \bth) &= \Pr(\bY \mid \bZ, \bmu, \bth)\Pr(\bZ \mid \bth)\nonumber\\
&= \prod^{N}_{i=1}\prod^{D}_{j=1}\Pr(Y_{ij} \mid \bZ, \bmu) \prod^N_{i=1}\Pr(Z_i \mid \bth)\nonumber\\
&= \prod^{N}_{i=1}\prod^{D}_{j=1}\prod^{K}_{k=1}\left\{\prod^{L}_{\ell=0}\pr{Y_{ij} = \ell | Z_i = k}\right\}^{\ind{Z_i=k}} \prod^N_{i=1}\prod^K_{k=1}\pr{Z_i = {k} \mid \bth}^{\ind{Z_i = k}}\nonumber \\
&= \prod^{N}_{i=1}\prod^{D}_{j=1}\prod^{K}_{k=1}\prod^{L}_{\ell=0}\pr{Y_{ij} = \ell \mid Z_{i} = k, \bmu}^{\ind{Y_{ij} = \ell, Z_i = k}} \prod^N_{i=1}\prod^K_{k=1}\pr{Z_i = {k} \mid \bth}^{\ind{Z_i = k}}\nonumber
\end{align}

Therefore, the complete log-likelihood is computed by taking the log of this:
\begin{align}
\mathcal{L}_{\text{comp}}(\bmu, \bth|\bY, \bZ)
& = \sum^{N}_{i=1}\sum^{D}_{j=1}\sum^{K}_{k=1}\sum^{L}_{\ell = 0}
\bm{1}\{Y_{ij} = \ell, Z_{i} = k\}\Big\{
\log \Pr(Y_{ij} = \ell | Z_{i} = k, \bm{\mu})
\Big\} \nonumber\\
& \qquad +
\sum^N_{i=1}\sum^{K}_{k=1}\bm{1}\{Z_{i} = k\}\log \Pr(Z_{i} = k | \bm{\theta})
\end{align}

We first take expectations over the latent variable $Z_{i}$,
\begin{align}
\E{\mathcal{L}_{\text{comp}}}
& = \sum^{N}_{i=1}\sum^{D}_{j=1}\sum^{K}_{k=1}\sum^{L}_{\ell = 0}
\bm{1}\{Y_{ij} = \ell\} \E{\bm{1}\{Z_{i} = k\}}\Big\{
\underbrace{\log \Pr(Y_{ij} = \ell | Z_{i} = k, \bm{\mu})}_{\equiv \log\bm{\mu}_{kj\ell}}
\Big\} \nonumber\\
& \qquad +
\sum^{N}_{i=1}\sum^{K}_{k=1}\E{\bm{1}\{Z_{i} = k\}}\underbrace{\log \Pr(Z_{i} = k | \bm{\theta})}_{= \log \bm{\theta}_{k}}
\end{align}

 Let's define this unknown quantity as 
\[\zeta_{ik} \equiv \E{\ind{Z_i = k}}.\]

Then the E-step can be the normalized version of the posterior probability marginalized by the mixing proportion,

\begin{align}
\widehat{\zeta}_{ij} \propto \bth_k\prod^D_{j = 1}\underbrace{\prod^{L}_{\ell = 0}\left(\bmu_{kj\ell}\right)^{\ind{Y_{ij} = \ell}}}_{\bmu_{kj, Y_{ij}}}
\end{align}

The M-step is derived by taking the derivatives of \(\E{\mathcal{L}_{\text{comp}}}\) with respect to the model parameters \(\bmu\) and \(\bth\). This leads to a MLE-like M-step

\begin{align}
\widehat{\theta}_{k} &= \frac{1}{N}\sum^N_{i = 1}\zeta_{ik}\\
\widehat{\mu}_{jk\ell} &= \frac{\sum^N_{i=1}\ind{Y_{ij} = \ell}\zeta_{ik}}{\sum^{N}_{i=1}\zeta_{ik}}
\end{align}


\paragraph{EM Implementation} We first need to set initial values for \(\bmu\) and \(\bth\). I do this by letting \(\bth^{(0)} = \left(\frac{1}{K},...\frac{1}{K}\right)\), randomly assigning an initial cluster assignment \(Z_i^\prime \sim \text{Cat}(\bth^{(0)})\), and setting the initial \(\bmu\) by the sample means of the data within those initial assignments, \(\mu_{jk}^{(0)} = \frac{\sum_{i=1}^{N}\ind{Y_{ij} = 1}\ind{Z_{i}^{\prime} = k}}{\sum_{i=1}^{N}\ind{Z_{i}^{\prime} = k}}.\) 

Then we iterate as follows. For each voter \(i\), compute the probability that they belong in cluster \(k\) (E-step):

\begin{align}
\zeta_{ik} \leftarrow \frac{\bm{\theta}_{k}\prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}}}
{\sum^{K}_{k^\prime=1}\bm{\theta}_{k^\prime}\prod^{D}_{j=1}\bm{\mu}_{k^\prime j,Y_{ij}}}
\end{align}

Then given those type probabilities, update the parameters with the MLE (M-step)

\begin{align}
\text{for each \(k\), update: }~~~  \widehat{\theta}_{k} &\leftarrow \frac{1}{N}\sum^N_{i = 1}\widehat{\zeta}_{ik} \\
\text{for each \(k, j, \ell\), update: }~~~ \widehat{\mu}_{jk\ell} &\leftarrow\frac{\sum^N_{i=1}\ind{Y_{ij} = \ell}\widehat{\zeta}_{ik}}{\sum^{N}_{i=1}\widehat{\zeta}_{ik}}
\end{align}

We repeat this until convergence.

\paragraph{Evaluating Convergence} We evaluate convergence by the observed log likelihood, 

\begin{align*}
\mathbf{L}_{\text{obs}} = \prod^N_{i=1}\sum^{K}_{k=1}\theta_k \prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}}
\end{align*}

So the log-likelihood is
\begin{align}
\mathcal{L}_{\text{obs}} = \sum^{N}_{i=1}\log \left\{\sum^{K}_{k=1}\theta_k \prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}}\right\} \label{eq:obsloglik}
\end{align}

Calculating eq. (\ref{eq:obsloglik}) is computationally intensive, so a quick way to check convergence is to track the maximum of the change in parameters which are all on the probability scale, i.e. \(\max\left\{|\widehat\theta^{(t + 1)}_{1} - \widehat\theta^{(t)}_{1}|, ..., |\widehat\mu^{(t + 1)}_{K,D} - \widehat\mu^{(t)}_{K,D}|\right\}\)

\paragraph{Speed-Ups} Because this EM algorithm deals with discrete data, the algorithm needs only sufficient statistics. In our setting the unique number of voting profiles is much smaller than the number of observations, because vote vectors follow a systematic pattern and most votes are straight-ticket votes. Therefore, we can re-format the dataset so that each row is a unique combination.

Let \(u \in \{1, ..., U\}\) index the unique voting profiles, and \(n_{u}\) be the number of such profiles in the data.  We re-cycle the objects \(\bY\) and \(\bm\zeta\) so that each row indexes profiles rather than voters.

We repeat the EM algorithm described earlier. For each profile \(u\), compute the probability that it belong in type \(k\):

\begin{align}
\text{for each \(u, k\), update: }~~~   \widehat\zeta_{uk} \leftarrow \frac{\bm{\theta}_{k}\prod^{D}_{j=1}\bm{\mu}_{kj,Y_{uj}}}
{\sum^{K}_{k^\prime=1}\bm{\theta}_{k^\prime}\prod^{D}_{j=1}\bm{\mu}_{k^\prime j,Y_{uj}}}
\end{align}

Then given those type probabilities, update with

\begin{align}
\text{for each \(k\), update: }~~~   \widehat{\theta}_{k} &\leftarrow \frac{1}{N}\sum^U_{u = 1}n_{u}\widehat{\zeta}_{uk}\\
\text{for each \(k, j, \ell\), update: }~~~   \widehat{\mu}_{jk\ell} &\leftarrow\frac{\sum^U_{u=1}n_{u}\ind{Y_{uj} = \ell}\widehat{\zeta}_{uk}}{\sum^{U}_{u=1}n_{u}\widehat{\zeta}_{uk}}
\end{align}

And the log-likelihood will also only require looping through the profiles:

So the log-likelihood is
\begin{align}
\mathcal{L}_{\text{obs}} &= \sum^{U}_{u=1}\log n_u + \sum^{U}_{u=1}\log\left\{\sum^{K}_{k=1}\theta_k \prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}}\right\}
\end{align}


\section{Modeling Uncontested Races} 

A majority of elections for state and local offices are uncontested, which means that a voter technically votes in a choice but does not have the option to vote for one of the candidates. These qualitatively-different settings for another layer of modeling. 

This setup uses the trichotomous outcome \(\ell \in \{0, 1,  2\}\) where \(Y_{ij} = 0\) indicates \emph{abstention}, \(Y_{ij} = 1\) indicates ticket \emph{splitting} (appearing like they are voting for their less preferred party as expressed in the top of the ticket) and \(Y_{ij} = 2\) indicates \emph{straight} (co-party) voting.  

\paragraph{Categories of uncontestedness} Now we introduce a new layer: voter \(i\) for a given office \(j\) is in one of three settings, denoted by \(M_{ij} \in \{1, 2, 3\}\). Unlike the cluster \(Z_i\), that status is exactly observed in the data. Denote \(M_{ij} = 3\) to mean vote \(j\) for voter \(i\) falls in the \emph{contested} case, so the voter has all three options on the menu. Denote \(M_{ij} = 2\) as the case when only the \emph{preferred party's} candidate is in the contest, so the voter only has options \(Y_{ij} \in \{0, 2\}\). Finally denote \(M_{ij} = 1\) as the case when only the \emph{opposed party's} candidate is in the contest, so the voter only has the option to abstain or ``reluctantly'' vote for the less favored option by splitting: \(Y_{i} \in \{0, 1\}\).

\paragraph{Moving from a Multinomial model to a logit model} To express the choice probability for option \(\ell\) for office \(j\) among voters of type \(k\), let us introduce another parameter \(\bps\) which represents the intensity of preference for option \(\ell \in \{1, 2\}\) relative to \(\ell = 0\) (abstention). We set the baseline for abstention to be 0, i.e. \(\psi_{jk,(\ell=0)} = 0 ~\forall~ j, k\).  Then the log-likelihood can be expressed in three parts. Formally, let \(V_{jk}(Y_{ij} = \ell)\) quantify the utility a voter gets for voting for voting for option \(\ell\).
 
\begin{align*}
\begin{cases}
\text{For }\ell = 0: & \psi_{jk0} \stackrel{\text{set}}{=} 0\\
\text{For }\ell \in \{1, 2\}: & \psi_{jk\ell} \equiv V_{jk}(Y_{ij} = \ell) - \psi_{jk\ell}\\
\end{cases}
\end{align*}

Of course, how much utility translates into vote choice is an empirical question, but for our purposes these are latent quantity anyways, so we will simply assume that \(\bps\) is an intercept which exclusively shapes the category.


\paragraph{Parameterization} To show the essence of this modeling choice, the below shows the case when there are no clusters \(K = 1\) and no latent heterogeneity across individuals for simplicity. Then the indices \(i, k\) drop and we get

\begin{align}
\mu_{j\ell} &= \ind{M_{j}= 1}\frac{\exp{\psi_{j\ell}}}{\sum_{\ell^\prime \in \{0, 1\}}\exp{\psi_{j\ell^\prime}}}\nonumber \\
&\qquad +\ind{M_{j}= 2}\frac{\exp{\psi_{j\ell}}}{\sum_{\ell^\prime \in \{0, 2\}}\exp{\psi_{j\ell^\prime}}}\nonumber \\
&\qquad\qquad +\ind{M_{j}= 3}\frac{\exp{\psi_{j\ell}}}{\sum_{\ell^\prime \in \{0, 1, 2\}}\exp{\psi_{j\ell^\prime}}}\label{eq:missing}
\end{align}

Now we add the clustering and individual indices back in for the real data, effectively another layer to account for the fact that individuals are both of a type (\(Z_i\)) and each separate office is also of a missingness type \(M_{ij}\):


\begin{align}
\mu_{jk\ell} &= \sum^K_{k=1}\ind{Z_i = k, M_{ij}= 1}\frac{\exp{\psi_{kj\ell}}}{\sum_{\ell^\prime \in \{0, 1\}}\exp{\psi_{kj\ell^\prime}}}\nonumber \\
&\qquad +\sum^K_{k=1}\ind{Z_i = k, M_{ij}= 2}\frac{\exp{\psi_{kj\ell}}}{\sum_{\ell^\prime \in \{0, 2\}}\exp{\psi_{kj\ell^\prime}}}\nonumber \\
&\qquad\qquad +\sum^K_{k=1}\ind{Z_i = k, M_{ij}= 3}\frac{\exp{\psi_{kj\ell}}}{\sum_{\ell^\prime \in \{0, 1, 2\}}\exp{\psi_{kj\ell^\prime}}}\label{eq:missing}
\end{align}


\paragraph{Analog to Multinomial Logit} Because \(\exp(\psi_{jk\ell}) = 1\) for \(\ell = 0\), which exists in all three components, each component is analogous to a simple multinomial logit. In the first two cases, since we consider only two possibilities, it reduces to a simple intercept-only logit. Also notice that we use the same set of parameters \(\bps_{jk}\) regardless of \(M_{ij}\). This represents the well-known independence of irrelevant alternatives (IIA) assumption in multinomial logit. The choice probabilities when one option is not on the ``menu'' is assumed to follow the same type of decision rule as the ratio between the existing options.  We can therefore think of equation \ref{eq:missing} as representing that the relevant parameter \(\mu\) for each individual takes one of the types depending on the (known) class of menu options, without introducing more parameters into the model.

\paragraph{EM Estimation} We can use this new representation of the parameter \(\mu\) in the EM algorithm. The three components in equation \ref{eq:missing} can be cast as a multinomial logit. R packages of multinomial logit typically presume IIA if an outcome value is missing and implicitly do the kind of three-way subsetting as in eq. \ref{eq:missing}. The required data would be of the ``long'' form

\begin{table}[!h]
\centering
\singlespacing
\begin{tabularx}{0.45\textwidth}{ccccc}
\toprule
Voter &  Office & \(M_i\) & Option $\ell$ &  \(Y_{ij}\)\\\midrule
\(i = 1\) & \(j = 1\) & 3 & 0 & \texttt{FALSE}\\
\(i = 1\) & \(j = 1\) & 3 & 1 & \texttt{FALSE}\\
\(i = 1\) & \(j = 1\) & 3 & 2 & \texttt{TRUE}\\\addlinespace
\(i = 1\) & \(j = 2\) & 2 & 0 & \texttt{FALSE}\\
\(i = 1\) & \(j = 2\) & 2 & 1 & \textcolor{crimson}{\texttt{NA}}\\
\(i = 1\) & \(j = 2\) & 2 & 2 & \texttt{TRUE}\\\addlinespace
\(i = 1\) & \(j = 3\) & 1 & 0 & \texttt{FALSE}\\
\(i = 1\) & \(j = 3\) & 1 & 1 & \texttt{TRUE}\\
\(i = 1\) & \(j = 3\) & 1 & 2 & \textcolor{crimson}{\texttt{NA}}\\
\bottomrule
\end{tabularx}
\end{table}

Here again, we've set aside the clustered nature. With that simplification, this data would allow for the simple syntax \texttt{m <- mlogit(Y $\sim$ 1, data = votes)}.  

We will estimate this regression separately for each \(k \in \{1, ..., K\}\), using the estimates of \(\widehat\zeta_{ik}\), which represents the posterior probability of voter \(i\) being in cluster \(k\), as the weights of the logit. 

We then read off the two coefficients (relative to baseline), which will determine the value for \(\mu_{jk\ell}\). 




\end{document}


