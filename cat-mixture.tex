\documentclass[11pt]{article}
\usepackage{sk_memo}

% Parmaters --
\title{ \Large\textbf{Clustering Voter Types with Multinomial Outcomes}}
\author{\normalsize  Shiro Kuriwaki\thanks{Thanks to Shusei Eshima, Max Goplerud, Sooahn Shin, and Soichiro Yamauchi for their generous time and help.} }

\date{\normalsize October 2019}


\begin{document}
\maketitle

\onehalfspacing


\section{Data Generating Process}


\paragraph{Setup}

Index individuals by \(i \in \{1, ..., N\}\) and the universe of races
excluding the top of the ticket as \(j \in \{1, ..., D\}\). The data we
observe is a length-\(D\) vector of votes \(\bY_i\). \(Y_{ij}\) is a
discrete response value, \(Y_{ij} \in \{0, ..., L\}\). For now, let's use \(L = 1\) so that each vote \(y_{ij}\) be a binary variable for splitting their ticket or not. \(Y_{ij} = 1\) would mean voter \(i\) splitting their ticket in some office \(j\), with reference to a top of the ticket office like the President or Governor.


\paragraph{Parameters}


Individuals are endowed with a cluster (or type) \(k \in \{1, ..., K\}\), which is drawn from a distribution governed by length-\(K\) simplex \(\bth\) (the mixing proportion).
\begin{align*}
Z_i \sim  \text{Cat}(\bth),
\end{align*}

Set \(\mu_{k, j} \in [0, 1]\) be the parameter that governs the outcome of each type. Therefore \(\bmu\) is a \(\{K \times D \times (L + 1)\}\) array, where 

 \[\pr{Y_{ij} = \ell \mid Z_i = k} = \mu_{kj\ell}.\]

In other words, for each individual (who is  type \(k\)), their observed vector \(\bY_{i}\) is governed by a length-\(D\) parameter \(\bmu_{k}\). Therefore, we can express the joint density as follows. 

\begin{align}
\pr{\bY_{i} \mid Z_{i} = k, \bth} = \prod^{D}_{j = 1} \text{Cat}(Y_{ij} | \bmu_k) = \prod^{D}_{j = 1}\prod^{L}_{\ell=0} \mu_{jk\ell}^{\ind{Y_{ij} = \ell}}
\end{align}

In our binary case, the Categorical reduces to a Bernoulli:

\begin{align}
\pr{\bY_{i} \mid Z_{i} = k, \bth} = \prod^{D}_{j=1}\mu_{jk}^{Y_{ij}}(1 - \mu_{jk})^{1- Y_{ij}}
\end{align}




The benefit of this modeling exercise over that from a naive sample of
\(N \times D\) Bernoullis is that we have captured the correlations
between variables.\footnote{That dependency can be expressed as \(\mathds{E}(\by) = \sum_{k = 1}^{K} \theta_{k} \bmu_{k}\) and 
\(\cov({\by}) = \sum_{k} \theta_{k} (\mathbf{\Sigma}_k \bmu_k\bmu_{k}^{\top}) - \mathds{E}(\by)\mathds{E}(\by)^\top\), where \(\Sigma_k = \text{diag}(\mu_{jk}(1 - \mu_{jk}))\).}


\section{EM}

This mixture model lends itself to clustering analysis such as \(K\)-means. Although we can estimate this model in a Bayesian fashion by setting a prior for \(\bth\) and \(\bmu\), the \textsf{Stan} program cannot reliably estimate clustering models like this one by MCMC because of label-switching and multimodality. 

Because the model is simple enough, we can derive an algorithm to obtain the global solution for the parameters.\footnote{Thanks to Soichiro Yamauchi for deriving this algorithm in the original iteration.} An EM implementation makes it possible to handle extensions, such as systematic missing data, multinomial outcomes, and covariates. 

\paragraph{Complete Likelihood} If we knew the cluster assignment, we would be able to write the complete likelihood (\(\mathbf{L}_{\text{comp}}\)) and the complete log-likelihood (\(\mathcal{L}_{\text{comp}}\)),


\begin{align}
\mathbf{L}_{\text{comp}}(\bmu, \bth | \bY, \bZ) =& \prod^{N}_{i=1}\prod^{D}_{j=1} \prod^{K}_{k = 1}\Pr(Z_i = k | \bth)  \prod^L_{\ell = 0}\Pr(Y_{ij}=  1 | Z_i = k, \bmu)^{\bm{1}(Y_{ij} = \ell \mid Z_i = k)}\nonumber\\
\mathcal{L}_{\text{comp}}(\bmu, \bth|\bY, \bZ)
& = \sum^{N}_{i=1}\sum^{D}_{j=1}\sum^{K}_{k=1}\sum^{2}_{\ell = 0}
\bm{1}\{Y_{ij} = \ell, Z_{i} = k\}\Big\{
\log \Pr(Y_{ij} = \ell | Z_{i} = k, \bm{\mu})
\Big\} \nonumber\\
& \qquad +
\sum^N_{i=1}\sum^{K}_{k=1}\bm{1}\{Z_{i} = k\}\log \Pr(Z_{i} = k | \bm{\theta})
\end{align}

We first take expectations over the latent variable $Z_{i}$,
\begin{align}
\E{\mathcal{L}_{\text{comp}}}
& = \sum^{N}_{i=1}\sum^{D}_{j=1}\sum^{K}_{k=1}\sum^{2}_{\ell = 0}
\bm{1}\{Y_{ij} = \ell\} \E{\bm{1}\{Z_{i} = k\}}\Big\{
\underbrace{\log \Pr(Y_{ij} = \ell | Z_{i} = k, \bm{\mu})}_{\equiv \log\bm{\mu}_{kj\ell}}
\Big\} \nonumber\\
& \qquad +
\sum^{N}_{i=1}\sum^{K}_{k=1}\E{\bm{1}\{Z_{i} = k\}}\underbrace{\log \Pr(Z_{i} = k | \bm{\theta})}_{= \log \bm{\theta}_{k}}
\end{align}

 Let's define this unkown quantity as 
\[\zeta_{ik} \equiv \E{\ind{Z_i = k}}.\]

Then the E-step can be the normalized version of the posterior probability marginalized by the mixing proportion,

\begin{align}
\widehat{\zeta}_{ij} \propto \bth_k\prod^D_{j = 1}\underbrace{\prod^{2}_{\ell = 0}\left(\bmu_{kj,\ell}\right)^{\ind{Y_{ij} = \ell}}}_{\bmu_{kj, Y_{ij}}}
\end{align}

The M-step is derived by taking the derivatives of \(\E{\mathcal{L}_{\text{comp}}}\) with respect to the model parameters \(\bmu\) and \(\bth\). This leads to a MLE-like M-step

\begin{align}
\widehat{\theta}_{k} &= \frac{1}{N}\sum^N_{i = 1}\zeta_{ik}\\
\widehat{\mu}_{jk\ell} &= \frac{\sum^N_{i=1}\ind{Y_{ij} = \ell}\zeta_{ik}}{\sum^{N}_{i=1}\zeta_{ik}}
\end{align}


\paragraph{EM Implementation} We first need to set initial values for \(\bmu\) and \(\bth\). I do this by letting \(\bth^{(0)} = \left(\frac{1}{K},...\frac{1}{K}\right)\), randomly assigning an initial cluster assignment \(Z_i^\prime \sim \text{Cat}(\bth^{(0)})\), and setting the initial \(\bmu\) by the sample means of the data within those initial assignments, \(\mu_{jk}^{(0)} = \frac{\sum_{i=1}^{N}\ind{Y_{ij} = 1}\ind{Z_{i}^{\prime} = k}}{\sum_{i=1}^{N}\ind{Z_{i}^{\prime} = k}}.\) 

Then we iterate as follows. For each voter \(i\), compute the probability that they belong in cluster \(k\) (E-step):

\begin{align}
\zeta_{ik} \leftarrow \frac{\bm{\theta}_{k}\prod^{D}_{j=1}\bm{\mu}_{kj,Y_{ij}}}
{\sum^{K}_{k^\prime=1}\bm{\theta}_{k^\prime}\prod^{D}_{j=1}\bm{\mu}_{k^\prime j,Y_{ij}}}
\end{align}

Then given those cluster probabilities, update the parameters with the MLE (M-step)

\begin{align}
\widehat{\theta}_{k} &\leftarrow \frac{1}{N}\sum^N_{i = 1}\widehat{\zeta}_{ik}\\
\widehat{\mu}_{jk1} &\leftarrow\frac{\sum^N_{i=1}\ind{Y_{ij} = 1}\widehat{\zeta}_{ik}}{\sum^{N}_{i=1}\widehat{\zeta}_{ik}}\\
\widehat{\mu}_{jk0} &= 1 - \widehat{\mu}_{jk1}
\end{align}

We repeat this until convergence.

\paragraph{Convergence} We evaluate convergence by the observed log likelihood, 

\begin{align*}
\mathbf{L}_{\text{obs}} &= \prod^{N}_{i=1}\sum^K_{k=1}\theta_k f_{k}(\bY_{i} | \bmu_{k})\\
&= \prod^N_{i=1}\sum^{K}_{k=1}\theta_k \prod^{D}_{j=1}\bm{\mu}_{k^\prime j,Y_{ij}}
\end{align*}

So the log-likelihood is
\begin{align}
\mathcal{L}_{\text{obs}} &= \sum^{N}_{i=1}\left[\prod^{K}_{k=1}\left\{\log\theta_{k} + \sum^D_{j=1}\sum^L_{\ell=1}\ind{Y_{ij} = \ell}\log(\mu_{kj\ell})\right\}\right]\label{eq:obsloglik}
\end{align}

Calculating eq. (\ref{eq:obsloglik}) is computationally intensive, so a quick way to check convergence is to track the maximum of the change in parameters which are all on the probability scale, i.e. \(\max\left\{|\widehat\theta^{(t + 1)}_{1} - \widehat\theta^{(t)}_{1}|, ..., |\widehat\mu^{(t + 1)}_{K,D} - \widehat\mu^{(t)}_{K,D}|\right\}\)

\paragraph{Speed-Ups} Because this EM algorithm deals with discrete data, the algorithm needs only sufficient statistics. In our setting the unique number of voting profiles is much smaller than the number of observations, because vote vectors follow a systematic pattern and most votes are straight-ticket votes. Therefore, we can re-format the dataset so that each row is a unique combination.

Let \(u \in \{1, ..., U\}\) index the unique voting profiles, and \(n_{u}\) be the number of such profiles in the data.  We re-cycle the objects \(\bY\) and \(\bm\zeta\) so that each row indexes profiles rather than voters.

We repeat the EM algorithm described earlier. For each profile \(u\), compute the probability that it belong in type \(k\):

\begin{align}
\widehat\zeta_{uk} \leftarrow \frac{\bm{\theta}_{k}\prod^{D}_{j=1}\bm{\mu}_{kj,Y_{uj}}}
{\sum^{K}_{k^\prime=1}\bm{\theta}_{k^\prime}\prod^{D}_{j=1}\bm{\mu}_{k^\prime j,Y_{uj}}}
\end{align}

Then given those type probabilities, update with

\begin{align}
\widehat{\theta}_{k} &\leftarrow \frac{1}{U}\sum^U_{u = 1}n_{u}\widehat{\zeta}_{uk}\\
\widehat{\mu}_{jk1} &\leftarrow\frac{\sum^U_{u=1}n_{u}\ind{Y_{uj} = 1}\widehat{\zeta}_{uk}}{\sum^{U}_{u=1}n_{u}\widehat{\zeta}_{uk}}\\
\end{align}

And the log-likelihood will also only require looping through the profiles:

So the log-likelihood is
\begin{align}
\mathcal{L}_{\text{obs}} &= \sum^{U}_{u=1}\left[n_u\prod^{K}_{k=1}\left\{\log\theta_{k} + \sum^D_{j=1}\sum^L_{\ell=1}\ind{Y_{uj} = \ell}\log(\mu_{kj\ell})\right\}\right]\label{eq:obsloglik}
\end{align}




\end{document}

